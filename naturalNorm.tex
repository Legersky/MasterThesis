\section{$\beta$-norm}

\begin{lem}
\label{lem:vectNorm}
Let $\nu$ be a norm of the vector space $\CC^d$ and $P$ be a nonsingular matrix in $\CC^d$. Then the mapping $\mu:\CC^d\rightarrow \RR^+_0$ defined by $\mu(x)=\nu(Px)$ is also a norm of the vector space $\CC^d$.
\end{lem}
\begin{proof}
Let $x$ and $y$ be vectors in $\CC^d$ and $\alpha\in \CC$.  We use linearity of matrix multiplication, nonsingularity of matrix $P$ and the fact that $\nu$ is a norm to prove the following statements:
\begin{enumerate}
    \item $\mu(x)=\nu(Px)\geq 0\,,$
    \item $\mu(x)=0 \iff \nu(Px)=0 \iff Px=0 \iff x=0\,,$
    \item $\mu(\alpha x)=\nu(P(\alpha x))=\nu(\alpha Px)=|\alpha|\nu(Px)=|\alpha|\mu(x)\,,$
    \item $\mu(x+y)=\nu(P(x+y))=\nu(Px+Py)\leq \nu(Px)+\nu(Py)=\mu(x)+\mu(y)\,.$
\end{enumerate}
This  verifies that $\mu$ is a norm.
\end{proof}


Lemma \ref{lem:vectNorm} enables us to define a new norm.
\begin{defn}
\label{def:newNorm}
Let $M\in\CC^{n\times n}$ be a diagonalizable matrix and $P\in\CC^{n\times n}$ be a nonsingular matrix which diagonalizes $M$, i.e., $M=P^{-1}DP$ for some diagonal matrix $D\in\CC^{n\times n}$. We define a vector norm $\norm{\cdot}{M}$ by  
\begin{equation}
\norm{x}{M}:=\norm{Px}{2}
\end{equation}
for all $x\in\CC^n$, where $\norm{\cdot}{2}$ is Euclidean norm. A matrix norm $\Mnorm{\cdot}{M}$ is induced by the norm $\norm{\cdot}{M}$.
\end{defn}

 

\begin{thm}
\label{thm:norm}
Let $M\in\CC^{n\times n}$ be a diagonalizable matrix. Then %re exists a vector norm $\norm{\cdot}{M}$ such that 
$$
\rho(M)=\Mnorm{M}{M}\,,
$$
where $\rho(M)$ is the spectral radius of the matrix $M$. % and $\Mnorm{\cdot}{M}$ is the natural matrix norm induced by $\norm{\cdot}{M}$.
\end{thm}
\begin{proof}
First, we prove that $\Mnorm{M}{M}\geq\rho(M)$. For all eigenvalues $\lambda$ in the spectrum $\sigma(M)$ with a respective eigenvector $u$ such that $\norm{u}{M}=1$, we have
$$
\Mnorm{M}{M}=\sup_{\norm{x}{M}=1} \norm{Mx}{M}\geq \norm{Mu}{M}=\norm{\lambda u}{M}=|\lambda|\cdot\norm{u}{M}=|\lambda|\,.
$$
Secondly, we show that $\Mnorm{M}{M}\leq\rho(M)$. Following Definition~\ref{def:newNorm}, let $P\in\CC^{n\times n}$ be a  nonsingular matrix  and $D\in\CC^{n\times n}$ a diagonal matrix  with the eigenvalues of $M$ on the diagonal such that $PMP^{-1}=D$.

Let $y$ be a  vector such that $\norm{y}{M}=1$ and set $z=Py$. Notice that 
$$
\sqrt{z^*z}=\norm{z}{2}=\norm{Py}{2}=\norm{y}{M}=1\,.
$$
Consider
\begin{align*}
\norm{My}{M}&=\norm{PMy}{2}=\norm{DPy}{2}=\norm{Dz}{2}=\sqrt{z^*D^*Dz}\\
    &\leq \sqrt{\max_{\lambda\in\sigma(M)}|\lambda|^2 z^*z}=\max_{\lambda\in\sigma(M)}|\lambda|=\rho(M)\,\,.
\end{align*}
which implies the statement.
\end{proof}

\begin{lem}
\label{lem:propertiesSbeta}
Let $\omega$ be an algebraic integer of degree $d$ and let $S$ be the companion matrix of its minimal polynomial $m_\omega$. Let $\beta=\sum_{i=0}^{d-1} b_i \omega^i$, where $b_i \in \ZZ$, be a nonzero element of $\Zomega$. Set $S_\beta=\multMat{b}$. Then
\begin{enumerate}[i)]
   \item The matrix $S_\beta$ is diagonalizable.
   \item The characteristic polynomial of $S_\beta$ is $m_\beta^k$ with $k=d / \deg \beta$.
   \item $|\det S_\beta|=|m_\beta(0)|^k$.
   \item $\norm{x}{S_\beta}=\norm{x}{S_\beta^{-1}}$ for all $x \in \CC^d$ and $\Mnorm{X}{S_\beta}=\Mnorm{X}{S_\beta^{-1}}$ for all $X \in \CC^{d\times d}$.
   \item $\Mnorm{S_\beta}{S_\beta}=\max \{|\beta'| \colon \beta' \text{ is conjugate of } \beta\}$ and $ \Mnorm{S_\beta^{-1}}{S_\beta}=\max \{\frac{1}{|\beta'|} \colon \beta' \text{ is conjugate of } \beta\}$.
\end{enumerate}  
\end{lem}
\begin{proof}
The characteristic polynomial of the companion matrix $S$ is the same as minimal polynomial of $\omega$ which has no multiple roots. Hence, $S$ is diagonalizable, i.e., $S=P^{-1}DP$ where $D$ is diagonal matrix with the conjugates of $\omega$ on the diagonal and $P$ is a nonsingular complex matrix. The matrix $S_\beta$ is also diagonalized by $P$:
$$
S_\beta=\sum_{i=0}^{d-1} b_i S^i= \sum_{i=0}^{d-1} b_i \left(P^{-1}DP\right)^i=P^{-1}\underbrace{\left(\sum_{i=0}^{d-1} b_i D^i\right)}_{D_\beta}P\,.
$$
By Theorem CONJUGATES SE ZOBRAZUJI NA CONJUGATES, the diagonal elements of the diagonal matrix $D_\beta$ are conjugates of $\beta$. Since $S_\beta\in\ZZ^{d\times d}$, its characteristic polynomial $p_{S_\beta}$ has integer coefficients. There exits $k\in\NN, k\geq 1$ such that $p_{S_\beta}=m^k_\beta$ as all roots of $p_{S_\beta}$ must be conjugates of $\beta$. The value $k$ follows from the equality $d=\deg(m_\beta^k)=k \deg m_\beta$. 

The modulus of the determinant of $S_\beta$ equals the modulus of the absolute coefficient of the characteristic polynomial $p_{S_\beta}$ which is $|m_\beta(0)|^k$.

The matrix $S_\beta^{-1}$ is also diagonalized by $P$ since $S_\beta^{-1}=(P^{-1}D_\beta P)^{-1}=P^{-1}D_\beta^{-1}P$. Thus, the norms $\norm{\cdot}{S_\beta}$ and $\norm{\cdot}{S_\beta^{-1}}$ are the same and so are the induced matrix norms $\Mnorm{\cdot}{S_\beta}$ and $\Mnorm{\cdot}{S_\beta^{-1}}$.

The matrix $S_\beta$ is diagonalizable and its eigenvalues are the conjugates of $\beta$. Theorem~\ref{thm:norm} implies that 
$$
\Mnorm{S_\beta}{S_\beta}=\rho(S_\beta)= \max \{|\beta'| \colon \beta' \text{ is conjugate of } \beta\}\,. 
$$
For the second part of the last statement, we use the part \textit{iv)}, Theorem~\ref{thm:norm} and the fact that the eigenvalues of $S_\beta^{-1}$ are  reciprocal of the conjugates of $\beta$.
\end{proof}

\begin{defn}
Let $\pi$ be the isomorphism between $\Zomega$ and $(\ZZ^d,+,\odot_\omega)$. Using the notation of the previous lemma, we define \emph{$\beta$-norm}  $\normBeta{\cdot}:\Zomega \rightarrow \RR^+_0$ by 
$$
\normBeta{x}=\norm{\pi(x)}{S_\beta}
$$
for all $x\in\Zomega$.
\end{defn}
We can easily verify that $\beta$-norm is a norm in $\Zomega$:
\begin{enumerate}
    \item $\normBeta{x}=\norm{\pi(x)}{S_\beta}\geq 0\,,$
    \item $\normBeta{x}=0 \iff \norm{\pi(x)}{S_\beta}=0 \iff \pi(x)=0 \iff x=0\,,$
    \item $\normBeta{\alpha x}=\norm{\pi(\alpha x)}{S_\beta}=|\alpha|\norm{\pi(x)}{S_\beta}=|\alpha|\normBeta{x}\,,$
    \item $\normBeta{x+y}=\norm{\pi(x+y)}{S_\beta}=\norm{\pi(x)+\pi(y)}{S_\beta}\leq \norm{\pi(x)}{S_\beta}+\norm{\pi(y)}{S_\beta}=\normBeta{x}+\normBeta{y}\,,$
\end{enumerate}
for all $x,y \in \Zomega$ and $\alpha \in \Zomega$.

